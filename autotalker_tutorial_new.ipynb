{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1594e9ba-c099-4915-acd4-db786c3b6434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.,  3., 12.])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(c, p=1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fea56f54-8160-4ae7-a14e-aee44ee1b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor([[ 1, 2, 8],[-5, 1, 4]] , dtype= torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53ddc820-e763-4416-8393-b747807a784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(idx, n_cls):\n",
    "    assert torch.max(idx).item() < n_cls\n",
    "    if idx.dim() == 1:\n",
    "        idx = idx.unsqueeze(1)\n",
    "    onehot = torch.zeros(idx.size(0), n_cls)\n",
    "    onehot = onehot.to(idx.device)\n",
    "    onehot.scatter_(1, idx.long(), 1)\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "869bf16f-d1d9-4bc8-a826-d5b9ae48df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57282df3-1f3c-469c-9be7-56566f0aa8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78082c7f-a0e5-4763-b9aa-2345b4b1fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test = torch.tensor([0, 2, 0])\n",
    "# one_hot_encoder(test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e5d3297-0290-47c0-846f-705e74cdb3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4aa3c6a-5a44-4cf1-84a3-5e514457cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "\n",
    "from autotalker.data import download_nichenet_ligand_target_mx\n",
    "from autotalker.data import load_spatial_adata_from_csv\n",
    "from autotalker.models import Autotalker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4d4e6d9-239f-47c5-a30b-579b16d4c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"squidpy_seqfish\"\n",
    "n_epochs = 10\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "n_hidden = 32\n",
    "n_latent = 16\n",
    "dropout_rate = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "726a55a8-65b2-49a2-b4f0-23cd69dc7954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset squidpy_seqfish.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using dataset {dataset}.\")\n",
    "\n",
    "if dataset == \"deeplinc_seqfish\":\n",
    "    adata = load_spatial_adata_from_csv(\"datasets/seqFISH/counts.csv\",\n",
    "                                        \"datasets/seqFISH/adj.csv\")\n",
    "    cell_type_key = None\n",
    "elif dataset == \"squidpy_seqfish\":\n",
    "    adata = sq.datasets.seqfish()\n",
    "    sq.gr.spatial_neighbors(adata, radius = 0.04, coord_type=\"generic\")\n",
    "    cell_type_key = \"celltype_mapped_refined\"\n",
    "elif dataset == \"squidpy_slideseqv2\":\n",
    "    adata = sq.datasets.slideseqv2()\n",
    "    sq.gr.spatial_neighbors(adata, radius = 30.0, coord_type=\"generic\")\n",
    "    cell_type_key = \"celltype_mapped_refined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d64513a5-d4f0-4ae2-927e-948c59bc8666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 19416\n",
      "Number of node features: 351\n",
      "Average number of edges per node: 4.4\n",
      "Number of edges: 42694\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of nodes: {adata.X.shape[0]}\")\n",
    "print(f\"Number of node features: {adata.X.shape[1]}\")\n",
    "avg_edges_per_node = round(\n",
    "    adata.obsp['spatial_connectivities'].toarray().sum(axis=0).mean(),2)\n",
    "print(f\"Average number of edges per node: {avg_edges_per_node}\")\n",
    "n_edges = int(np.triu(adata.obsp['spatial_connectivities'].toarray()).sum())\n",
    "print(f\"Number of edges: {n_edges}\", sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0145cc84-0dae-4b94-831c-3637d9d0fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = mlflow.set_experiment(\"autotalker\")\n",
    "mlflow.log_param(\"dataset\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06530d4e-020b-4fdc-89cb-e16daa410205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask that allows all genes\n",
    "mask = np.ones((n_latent, len(adata.var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5431a94f-3159-4556-98d5-551a490f161f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING NEW NETWORK MODULE..........\n",
      "GCN ENCODER - n_input: 351, n_hidden: 32, n_latent: 16, dropout_rate: 0.0\n",
      "DOT PRODUCT GRAPH DECODER - dropout_rate: 0.0\n",
      "MASKED LINEAR EXPRESSION DECODER - n_input: 16, n_output: 351, n_condition: 1, n_extension_unmasked: 0, n_extension_masked: 0, recon_loss: nb\n"
     ]
    }
   ],
   "source": [
    "model = Autotalker(adata,\n",
    "                   mask=mask,\n",
    "                   n_hidden=n_hidden,\n",
    "                   n_latent=n_latent,\n",
    "                   dropout_rate=dropout_rate,\n",
    "                   expr_decoder_recon_loss=\"nb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fa100cc-c398-4ed9-a72f-a42af0d8c16d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4328x15 and 16x351)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmlflow_experiment_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sebastianbirk/autotalker/autotalker/models/_autotalker.py:128\u001b[0m, in \u001b[0;36mAutotalker.train\u001b[0;34m(self, n_epochs, lr, weight_decay, val_frac, test_frac, mlflow_experiment_id, **trainer_kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mTrain the model.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    Kwargs for the trainer.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m Trainer(adata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madata,\n\u001b[1;32m    121\u001b[0m                        model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    122\u001b[0m                        condition_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcondition_key_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m                        mlflow_experiment_id\u001b[38;5;241m=\u001b[39mmlflow_experiment_id,\n\u001b[1;32m    127\u001b[0m                        \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_kwargs)\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_trained_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/sebastianbirk/autotalker/autotalker/train/_trainer.py:153\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_epochs, lr, weight_decay, mlflow_experiment_id)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader:\n\u001b[1;32m    152\u001b[0m     train_data_batch \u001b[38;5;241m=\u001b[39m train_data_batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_epoch_end()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_early_stopping:\n",
      "File \u001b[0;32m~/sebastianbirk/autotalker/autotalker/train/_trainer.py:185\u001b[0m, in \u001b[0;36mTrainer.on_iteration\u001b[0;34m(self, train_data_batch)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_iteration\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_data_batch):\n\u001b[0;32m--> 185\u001b[0m     adj_recon_logits, x_recon, mu, logstd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     train_loss, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss(adj_recon_logits,\n\u001b[1;32m    189\u001b[0m                                        x_recon,\n\u001b[1;32m    190\u001b[0m                                        train_data_batch,\n\u001b[1;32m    191\u001b[0m                                        mu,\n\u001b[1;32m    192\u001b[0m                                        logstd,\n\u001b[1;32m    193\u001b[0m                                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_logs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/autotalker/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/sebastianbirk/autotalker/autotalker/modules/_ivgae.py:79\u001b[0m, in \u001b[0;36mIVGAE.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogstd)\n\u001b[1;32m     78\u001b[0m adj_recon_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_decoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz)\n\u001b[0;32m---> 79\u001b[0m expr_decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adj_recon_logits, expr_decoder_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogstd\n",
      "File \u001b[0;32m~/anaconda3/envs/autotalker/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/sebastianbirk/autotalker/autotalker/nn/_decoders.py:92\u001b[0m, in \u001b[0;36mMaskedLinearExprDecoder.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m---> 92\u001b[0m     x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_l0(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmce_l0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_recon\n",
      "File \u001b[0;32m~/anaconda3/envs/autotalker/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/sebastianbirk/autotalker/autotalker/nn/_layers.py:134\u001b[0m, in \u001b[0;36mMaskedCondExtLayer.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28minput\u001b[39m, extension_masked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28minput\u001b[39m, [\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextension_masked,\n\u001b[1;32m    130\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextension_masked],\n\u001b[1;32m    131\u001b[0m         dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Forward pass with different layer components\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_l\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extension_unmasked \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextension_unmasked_l(extension_unmasked)\n",
      "File \u001b[0;32m~/anaconda3/envs/autotalker/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/sebastianbirk/autotalker/autotalker/nn/_layer_components.py:22\u001b[0m, in \u001b[0;36mMaskedLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 22\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4328x15 and 16x351)"
     ]
    }
   ],
   "source": [
    "model.train(n_epochs=n_epochs,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "            mlflow_experiment_id=experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1499445-5ec0-4c1b-b08b-40c94ace8894",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(dir_path=\"./model_artefacts\",\n",
    "           overwrite=True,\n",
    "           save_adata=True,\n",
    "           adata_file_name=\"adata.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68cc58d-54c7-4f7b-8420-1064f7c288fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autotalker.load(dir_path=\"./model_artefacts\",\n",
    "                        adata=None,\n",
    "                        adata_file_name=\"adata.h5ad\",\n",
    "                        use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe5d54-2260-44ad-a024-3e9c420453ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = model.get_latent_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1bf9d5-90de-4898-96b7-697e1c3de254",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_new_data = model.get_latent_representation(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed32da-ccc5-49af-a815-239791ad2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm[\"latent_autotalker\"] = latent_new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85936c-e256-4325-b056-a27cb8ba3478",
   "metadata": {},
   "source": [
    "## Interoperability with scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd90a8c5-9248-469e-ba7c-3f8f17add24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use autotalker latent space for UMAP generation\n",
    "sc.pp.neighbors(adata, use_rep=\"latent_autotalker\")\n",
    "sc.tl.umap(adata, min_dist=0.3)\n",
    "sc.pl.umap(adata, color=[\"celltype_mapped_refined\"], frameon=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2304a-6700-46dc-b697-1bf38ede2848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d4732-14f1-47bb-a18e-f585fcd6d860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2bb33-15a2-4725-9b9d-5dd2dd49c056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ca973-1086-494c-a877-877e1e2e643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_nichenet_ligand_target_mx(save_path=\"data/ligand_target_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006bc09-cffc-45b1-96e3-83b218496411",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ec93b-dd53-4cbd-a9e2-9ae49474a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autotalker(adata=adata,\n",
    "                   mask=mask,\n",
    "                   n_hidden=n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006a1e8-fdab-497d-8a3e-f450204b1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = mlflow.set_experiment(\"autotalker\")\n",
    "mlflow.log_param(\"dataset\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5b9aa-cdea-4408-842e-217d6e5d11c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(n_epochs=n_epochs,\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "            mlflow_experiment_id=experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f1b1bf-6271-4a03-ab68-b5c147b338cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ea18b-4cc8-4642-b2f2-5ff0c7f511c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotalker",
   "language": "python",
   "name": "autotalker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
